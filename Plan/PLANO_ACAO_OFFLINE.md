# Plano de A√ß√£o para Robustez da Arquitetura Offline-First

## üìã Diagn√≥stico Geral

A arquitetura offline-first √© bem estruturada e funcional em seu n√∫cleo. No entanto, a an√°lise aprofundada do c√≥digo revelou **pontos cr√≠ticos de risco** que podem levar a **perda de dados silenciosa, inconsist√™ncias e bugs** para os encarregados.

A causa raiz da maioria dos problemas √© que a l√≥gica de sincroniza√ß√£o (`syncService`) e a fila de opera√ß√µes (`QueueService`) **n√£o utilizam o `ConflictResolver`**, que j√° est√° implementado, resultando em uma estrat√©gia de "o √∫ltimo a escrever vence" sem valida√ß√£o.

---

## üí£ Principais Riscos e Potenciais Bugs

1.  **[RISCO ALTO] Perda de Dados por Sobrescrita:**
    *   **Cen√°rio:** Um encarregado edita um servi√ßo offline. Ao mesmo tempo, um gestor edita o mesmo servi√ßo no sistema web. Quando o encarregado fica online, sua vers√£o **sobrescreve** a do gestor, perdendo as altera√ß√µes feitas.
    *   **Causa:** `syncService` e `QueueService` usam `supabase.upsert()` e `supabase.update()` diretamente, sem antes verificar conflitos com os dados do servidor.

2.  **[RISCO M√âDIO] Dados "Fantasma" no Dispositivo:**
    *   **Cen√°rio:** Um servi√ßo √© exclu√≠do no sistema web. No entanto, ele **permanece vis√≠vel** no aplicativo do encarregado indefinidamente, pois n√£o h√° sincroniza√ß√£o de exclus√µes.
    *   **Causa:** A l√≥gica de `syncFromServer` apenas insere ou atualiza registros (`INSERT OR REPLACE`), mas nunca remove registros que foram deletados no servidor.

3.  **[RISCO M√âDIO] Loop Infinito de Opera√ß√µes Falhas:**
    *   **Cen√°rio:** Uma opera√ß√£o offline falha ao ser enviada ao servidor (ex: viola√ß√£o de uma regra de neg√≥cio). O `QueueService` tentar√° reenviar essa opera√ß√£o **infinitamente** toda vez que o app ficar online, consumindo recursos e potencialmente bloqueando outras opera√ß√µes.
    *   **Causa:** O `QueueService` incrementa o n√∫mero de tentativas, mas n√£o existe um limite m√°ximo para parar de tentar e marcar a opera√ß√£o como falha permanente.

4.  **[RISCO BAIXO] Inconsist√™ncia de Dados por Fuso Hor√°rio:**
    *   **Cen√°rio:** No final ou in√≠cio do dia, um encarregado pode n√£o ver todos os servi√ßos planejados para a data atual.
    *   **Causa:** A query em `OfflineDataService` para buscar servi√ßos do dia (`WHERE DATE(s.data_planejada) = ?`) √© sens√≠vel a configura√ß√µes de fuso hor√°rio do dispositivo e do servidor, podendo resultar em inconsist√™ncias.

---

## üõ†Ô∏è Plano de A√ß√£o Corretivo

O foco √© integrar os servi√ßos existentes para que funcionem em harmonia, com o m√≠nimo de c√≥digo novo.

### Fase 1: Integra√ß√£o do `ConflictResolver` (Prioridade Alta)

**Objetivo:** Eliminar o risco de perda de dados.

1.  **Modificar `syncService.ts`:**
    *   Antes de executar o `supabase.upsert()`, o servi√ßo **deve** primeiro buscar a vers√£o atual do registro no servidor.
    *   Comparar o registro local (`servico`) com o do servidor usando o `ConflictResolver.resolveConflict(local, server)`.
    *   Enviar para o Supabase apenas o `resolvedData` retornado pelo resolvedor.
    *   Logar o conflito usando `ConflictResolver.logConflict()` se `hasConflict` for `true`.

    **Exemplo de implementa√ß√£o em `syncToServer`:**
    ```typescript
    // Dentro do loop 'for (const servico of servicosToSync)'
    
    // 1. Buscar vers√£o do servidor
    const { data: serverServico } = await supabase
      .from('servicos')
      .select('*')
      .eq('id', servico.id)
      .single();

    if (serverServico) {
      // 2. Resolver conflito se existir
      const resolver = new ConflictResolver();
      const resolution = resolver.resolveConflict(servico, serverServico, 'last_modified');

      if (resolution.hasConflict) {
        await ConflictResolver.logConflict({
          tableName: 'servicos',
          recordId: servico.id,
          resolution: resolution.strategy,
          winner: resolution.resolvedData,
          loser: resolution.resolvedData.id === servico.id ? serverServico : servico,
        });
      }
      
      // 3. Enviar o dado resolvido
      const { error } = await supabase.from('servicos').upsert(resolution.resolvedData);
      // ... resto da l√≥gica
    } else {
      // Se n√£o existe no servidor, √© um novo registro, pode inserir direto
      const { error } = await supabase.from('servicos').insert(servico);
      // ...
    }
    ```

2.  **Modificar `QueueService.ts`:**
    *   Aplicar a **mesma l√≥gica** descrita acima dentro do `executeOperation`. Antes de um `UPDATE`, buscar a vers√£o do servidor, resolver o conflito e ent√£o executar a opera√ß√£o com os dados resolvidos.

### Fase 2: Melhorar a Fila e Sincroniza√ß√£o (Prioridade M√©dia)

**Objetivo:** Aumentar a resili√™ncia do sistema.

1.  **Implementar Limite de Tentativas no `QueueService.ts`:**
    *   Adicionar uma verifica√ß√£o no `incrementAttempts`. Se `attempts + 1` exceder um limite (ex: 5), mudar o status da opera√ß√£o para `'FAILED'`.
    *   No `processQueue`, ignorar opera√ß√µes com status `'FAILED'`.

2.  **Implementar Sincroniza√ß√£o de Exclus√µes:**
    *   Esta √© a parte mais complexa. Uma estrat√©gia comum √© usar "soft deletes" (marcar um registro como `deleted_at` em vez de remov√™-lo).
    *   **No Supabase:** Adicionar uma coluna `deleted_at` √†s tabelas. Em vez de `DELETE`, a aplica√ß√£o web faria um `UPDATE` para setar esta data.
    *   **No `syncFromServer`:** Sincronizar os registros que t√™m `deleted_at` preenchido e remov√™-los do SQLite local.

### Fase 3: Refinamentos e Testes (Cont√≠nuo)

**Objetivo:** Garantir a estabilidade e a qualidade do c√≥digo.

1.  **Refatorar `OfflineDataService.ts`:**
    *   Remover a fun√ß√£o `getServicos` duplicada.
    *   Padronizar o mapeamento de dados entre `getServicos` e `getServicosByEncarregado` para evitar inconsist√™ncias na UI.
    *   Para a query de data, considere buscar os servi√ßos em um intervalo de tempo maior (ex: D-1, D, D+1) e fazer o filtro final no lado do cliente para mitigar problemas de fuso hor√°rio.

2.  **Implementar Testes Automatizados:**
    *   Como j√° recomendado, criar testes unit√°rios e de integra√ß√£o √© **crucial**.
    *   **Cen√°rios de teste priorit√°rios:**
        *   Um registro √© modificado offline e sincronizado com sucesso.
        *   Um conflito de dados ocorre e √© resolvido corretamente pela estrat√©gia `last_modified`.
        *   Uma opera√ß√£o na fila falha 5 vezes e √© marcada como `'FAILED'`.
        *   Um registro deletado no servidor √© removido do app local ap√≥s a sincroniza√ß√£o.

---

## üîö Conclus√£o

Sua implementa√ß√£o offline est√° a um passo de ser excelente. Ao focar em **integrar o `ConflictResolver`** e adicionar **resili√™ncia ao `QueueService`**, voc√™ eliminar√° os riscos mais graves e garantir√° que os encarregados possam trabalhar offline com seguran√ßa e sem perda de dados.
